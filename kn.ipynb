{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    # tokens = re.findall(r'\\b\\w+\\b|[^\\w\\s]', text)\n",
    "    s = text\n",
    "    s = re.sub('\\#[a-zA-Z]\\w+', '<HASHTAG>', s)\n",
    "    s = re.sub(r'\\S*[\\w\\~\\-]\\@[\\w\\~\\-]\\S*', r'<EMAIL>', s)\n",
    "    s = re.sub(r'(https?:\\/\\/|www\\.)?\\S+[a-zA-Z0-9]{2,}\\.[a-zA-Z0-9]{2,}\\S+', r'<URL>', s)\n",
    "    s = re.sub('@\\w+', '<MENTION>', s)\n",
    "    s = s.lower()\n",
    "    s = re.sub('\\[.*\\s+-\\s+.*\\]', '<FOOTNOTE>', s)\n",
    "\n",
    "    s = re.sub(r'(!|\"|\\#|\\$|%|&|\\'|\\(|\\)|\\*|\\+|,|-|\\.|\\/|:|;|<|=|>|\\?|@|\\[|\\\\|\\]|\\^|_|‘|\\{|\\||\\}|~)\\1{1,}', r'\\1', s)\n",
    "    s = re.sub(r'\\d{2,4}\\-\\d\\d-\\d{2,4}|\\d{2,4}\\/\\d\\d\\/\\d{2,4}|\\d{2,4}:\\d\\d:?\\d{2,4}', '<DATE>', s)\n",
    "    s = re.sub(r'\\d+:\\d\\d:?\\d{0,2}?( am|am| pm|pm)', r'<TIME>', s)\n",
    "    s = re.sub(r'[\\+0-9\\-\\(\\)\\.]{3,}[\\-\\.]?[0-9\\-\\.]{3,}', r'<MOB>', s)\n",
    "    s = re.sub(r'(?<=\\s)[\\:\\.]?\\d*[\\:\\.]?\\d*[\\:\\.]?(?=\\s)', r'<NUM>', s)\n",
    "    s = re.sub(r'(\\w)\\1{2,}', r'\\1\\1', s)\n",
    "    s = re.sub(r'can\\'t', r'can not', s)\n",
    "    s = re.sub(r'won\\'t', r'will not', s)\n",
    "    s = re.sub(r'([a-zA-Z]+)-([a-zA-Z]+)', r'\\1 \\2', s)\n",
    "    \n",
    "    # Separating placeholders\n",
    "\n",
    "    tokens = s.split()\n",
    "    return tokens\n",
    "\n",
    "def addStartEnd(tokens, n):\n",
    "    for i in range(n):\n",
    "        tokens.insert(0, \"<start>\")\n",
    "        tokens.append(\"</start>\")\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanCorpus(corpus, fileName):\n",
    "    # print(corpus)\n",
    "    # corpus = re.sub(r'http\\S+', '<URL>', corpus)\n",
    "    # corpus = re.sub(r'#\\S+', '<HASHTAG>', corpus)\n",
    "    # corpus = re.sub(r'@\\S+', '<MENTION>', corpus)\n",
    "    # corpus = corpus.lower()\n",
    "    for i in range(len(corpus)): \n",
    "        s = corpus[i]   \n",
    "        s = re.sub('\\#[a-zA-Z]\\w+', '<HASHTAG>', s)\n",
    "        s = re.sub(r'\\S*[\\w\\~\\-]\\@[\\w\\~\\-]\\S*', r'<EMAIL>', s)\n",
    "        s = re.sub(r'(https?:\\/\\/|www\\.)?\\S+[a-zA-Z0-9]{2,}\\.[a-zA-Z0-9]{2,}\\S+', r'<URL>', s)\n",
    "        s = re.sub('@\\w+', '<MENTION>', s)\n",
    "        s = s.lower()\n",
    "        s = re.sub('\\[.*\\s+-\\s+.*\\]', '<FOOTNOTE>', s)\n",
    "        s = re.sub(r'(!|\"|\\#|\\$|%|&|\\'|\\(|\\)|\\*|\\+|,|-|\\.|\\/|:|;|<|=|>|\\?|@|\\[|\\\\|\\]|\\^|_|‘|\\{|\\||\\}|~)\\1{1,}', r'\\1', s)\n",
    "        s = re.sub(r'\\d{2,4}\\-\\d\\d-\\d{2,4}|\\d{2,4}\\/\\d\\d\\/\\d{2,4}|\\d{2,4}:\\d\\d:?\\d{2,4}', '<DATE>', s)\n",
    "        s = re.sub(r'\\d+:\\d\\d:?\\d{0,2}?( am|am| pm|pm)', r'<TIME>', s)\n",
    "        s = re.sub(r'[\\+0-9\\-\\(\\)\\.]{3,}[\\-\\.]?[0-9\\-\\.]{3,}', r'<MOB>', s)\n",
    "        s = re.sub(r'(?<=\\s)[\\:\\.]?\\d*[\\:\\.]?\\d*[\\:\\.]?(?=\\s)', r'<NUM>', s)\n",
    "        s = re.sub(r'(\\w)\\1{2,}', r'\\1\\1', s)\n",
    "        s = re.sub(r'can\\'t', r'can not', s)\n",
    "        s = re.sub(r'won\\'t', r'will not', s)\n",
    "        s = re.sub(r'([a-zA-Z]+)-([a-zA-Z]+)', r'\\1 \\2', s)\n",
    "        corpus[i] = s \n",
    "   \n",
    "    try: \n",
    "        os.mkdir('cleanCorpus')\n",
    "    except:\n",
    "        # do nothing\n",
    "        pass\n",
    "    \n",
    "    newFile = 'cleanCorpus/' + fileName\n",
    "    # delete file if it exists\n",
    "    try:\n",
    "        os.remove(newFile)\n",
    "    except:\n",
    "        # do nothing\n",
    "        pass\n",
    "    f = open(newFile, 'x')\n",
    "    for i in range(len(corpus)):\n",
    "        f.write(corpus[i])\n",
    "    f.close()\n",
    "    \n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "fileName1 = 'Ulysses - James Joyce.txt'\n",
    "fileName2 = 'Pride and Prejudice - Jane Austen.txt'\n",
    "\n",
    "\n",
    "string2 = 'corpus/' + fileName1\n",
    "with open(string2, 'r') as f:\n",
    "    corpus1 = f.readlines()\n",
    "corpus1 = cleanCorpus(corpus1, fileName1)\n",
    "\n",
    "string4 = 'corpus/' + fileName2\n",
    "with open(string4, 'r') as f:\n",
    "    corpus2 = f.readlines()\n",
    "corpus2 = cleanCorpus(corpus2, fileName2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perplexityScore(prob):\n",
    "\n",
    "    # return perplexity score for list of probabilities\n",
    "    sum = 0\n",
    "    for p in prob:\n",
    "        if p != 0:\n",
    "            sum += math.log(p)\n",
    "        else:\n",
    "            sum += math.log(1e-10)\n",
    "\n",
    "    return math.exp(-sum/len(prob))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getFrequencyCount(trainSample):\n",
    "    freqCountFullString = [{}, {}, {}, {}]\n",
    "    unigramCount = {}\n",
    "    bigramCount = {}\n",
    "    trigramCount = {}\n",
    "    quadgramCount = {}\n",
    "    for sentence in trainSample:\n",
    "        # print(sentence)\n",
    "        tokens = tokenize(sentence)\n",
    "        for token in tokens:\n",
    "            if token not in unigramCount:\n",
    "                unigramCount[token] = 1\n",
    "            else:\n",
    "                unigramCount[token] += 1\n",
    "    \n",
    "    temp = 0\n",
    "    wordsToReplace = []\n",
    "    for key, value in unigramCount.copy().items():\n",
    "        if value < 10:\n",
    "            temp += value\n",
    "            unigramCount.pop(key)\n",
    "            wordsToReplace.append(key)\n",
    "    \n",
    "    unigramCount[\"<unk>\"] = temp\n",
    "\n",
    "    print(\"Unknowns identified\")\n",
    "\n",
    "    for index in range(len(trainSample)):\n",
    "        #print(\"Sentence \" + str(index) + \" of \" + str(len(trainSample)))\n",
    "        tokens = tokenize(trainSample[index])\n",
    "        for i in range(len(tokens)):\n",
    "            if tokens[i] in wordsToReplace:\n",
    "                tokens[i] = \"<unk>\"\n",
    "        trainSample[index] = ' '.join(tokens)\n",
    "        \n",
    "    freqCountFullString[0] = unigramCount\n",
    "    \n",
    "    print(\"Unknowns replaced\")\n",
    "\n",
    "    for index in range(len(trainSample)):\n",
    "        sentence = trainSample[index]\n",
    "        tokens = tokenize(sentence)\n",
    "        tokens = addStartEnd(tokens, 3)\n",
    "        \n",
    "        for i in range(len(tokens) - 1):\n",
    "            if tokens[i] not in bigramCount:\n",
    "                bigramCount[tokens[i]] = {}\n",
    "            if tokens[i + 1] not in bigramCount[tokens[i]]:\n",
    "                bigramCount[tokens[i]][tokens[i + 1]] = 1\n",
    "            else:\n",
    "                bigramCount[tokens[i]][tokens[i + 1]] += 1\n",
    "            \n",
    "            currBigram = tokens[i] + \" \" + tokens[i + 1]\n",
    "            \n",
    "            if currBigram not in freqCountFullString[1]:\n",
    "                freqCountFullString[1][currBigram] = 1\n",
    "            else:\n",
    "                freqCountFullString[1][currBigram] += 1\n",
    "\n",
    "        \n",
    "        # tokens = addStartEnd(tokens, 1)\n",
    "\n",
    "        for i in range(len(tokens) - 2):\n",
    "            if tokens[i] not in trigramCount:\n",
    "                trigramCount[tokens[i]] = {}\n",
    "            if tokens[i + 1] not in trigramCount[tokens[i]]:\n",
    "                trigramCount[tokens[i]][tokens[i + 1]] = {}\n",
    "            if tokens[i + 2] not in trigramCount[tokens[i]][tokens[i + 1]]:\n",
    "                trigramCount[tokens[i]][tokens[i + 1]][tokens[i + 2]] = 1\n",
    "            else:\n",
    "                trigramCount[tokens[i]][tokens[i + 1]][tokens[i + 2]] += 1\n",
    "            \n",
    "            currTrigram = tokens[i] + \" \" + tokens[i + 1] + \" \" + tokens[i + 2]\n",
    "\n",
    "            if currTrigram not in freqCountFullString[2]:\n",
    "                freqCountFullString[2][currTrigram] = 1\n",
    "            else:\n",
    "                freqCountFullString[2][currTrigram] += 1\n",
    "\n",
    "        #tokens = addStartEnd(tokens, 1)\n",
    "\n",
    "        for i in range(len(tokens) - 3):\n",
    "            if tokens[i] not in quadgramCount:\n",
    "                quadgramCount[tokens[i]] = {}\n",
    "            if tokens[i + 1] not in quadgramCount[tokens[i]]:\n",
    "                quadgramCount[tokens[i]][tokens[i + 1]] = {}\n",
    "            if tokens[i + 2] not in quadgramCount[tokens[i]][tokens[i + 1]]:\n",
    "                quadgramCount[tokens[i]][tokens[i + 1]][tokens[i + 2]] = {}\n",
    "            if tokens[i + 3] not in quadgramCount[tokens[i]][tokens[i + 1]][tokens[i + 2]]:\n",
    "                quadgramCount[tokens[i]][tokens[i + 1]][tokens[i + 2]][tokens[i + 3]] = 1\n",
    "            else:\n",
    "                quadgramCount[tokens[i]][tokens[i + 1]][tokens[i + 2]][tokens[i + 3]] += 1\n",
    "            \n",
    "            currQuadgram = tokens[i] + \" \" + tokens[i + 1] + \" \" + tokens[i + 2] + \" \" + tokens[i + 3]\n",
    "            \n",
    "            if currQuadgram not in freqCountFullString[3]:\n",
    "                freqCountFullString[3][currQuadgram] = 1\n",
    "            else:\n",
    "                freqCountFullString[3][currQuadgram] += 1\n",
    "        \n",
    "        trainSample[index] = ' '.join(tokens)\n",
    "\n",
    "    # print(trainSample)\n",
    "    freqCount = [unigramCount, bigramCount, trigramCount, quadgramCount]\n",
    "    return freqCount, freqCountFullString\n",
    "\n",
    "def returnCount(string, freqCount):\n",
    "    if(len(string) == 0):\n",
    "        return 0\n",
    "    lenString = len(string.split(\" \"))\n",
    "    tokens = tokenize(string)\n",
    "    try:\n",
    "        if(lenString == 1):\n",
    "            return freqCount[0][tokens[0]]\n",
    "        elif(lenString == 2):\n",
    "            return freqCount[1][tokens[0]][tokens[1]]\n",
    "        elif(lenString == 3):\n",
    "            return freqCount[2][tokens[0]][tokens[1]][tokens[2]]\n",
    "        elif(lenString == 4):\n",
    "            return freqCount[3][tokens[0]][tokens[1]][tokens[2]][tokens[3]]\n",
    "    except KeyError:\n",
    "        return 0\n",
    "\n",
    "def returnPositiveCount(history, freqCount):\n",
    "    retVal = 0\n",
    "    n = len(history.split(\" \")) + 1\n",
    "    tokens = tokenize(history)\n",
    "\n",
    "    try:\n",
    "        if(n == 1):\n",
    "            for key, val in freqCount[0].items():\n",
    "                if(val > 0):\n",
    "                    retVal += 1\n",
    "        elif(n == 2):\n",
    "            for key, val in freqCount[1][tokens[0]].items():\n",
    "                if(val > 0):\n",
    "                    retVal += 1\n",
    "        elif(n == 3):\n",
    "            for key, val in freqCount[2][tokens[0]][tokens[1]].items():\n",
    "                if(val > 0):\n",
    "                    retVal += 1\n",
    "        elif(n == 4):\n",
    "            for key, val in freqCount[3][tokens[0]][tokens[1]][tokens[2]].items():\n",
    "                if(val > 0):\n",
    "                    retVal += 1\n",
    "    except:\n",
    "        return 0\n",
    "    return retVal\n",
    "\n",
    "def returnSumCount(history, freqCount):\n",
    "    retVal = 0\n",
    "    n = len(history.split(\" \")) + 1\n",
    "    tokens = tokenize(history)\n",
    "    try:\n",
    "        if(n == 1):\n",
    "            for key, val in freqCount[0].items():\n",
    "                retVal += val\n",
    "        elif(n == 2):\n",
    "            for key, val in freqCount[1][tokens[0]].items():\n",
    "                retVal += val\n",
    "        elif(n == 3):\n",
    "            for key, val in freqCount[2][tokens[0]][tokens[1]].items():\n",
    "                retVal += val\n",
    "        elif(n == 4):\n",
    "            for key, val in freqCount[3][tokens[0]][tokens[1]][tokens[2]].items():\n",
    "                retVal += val\n",
    "    except:\n",
    "        return 0\n",
    "    return retVal\n",
    "\n",
    "def continuationCount(fullFreqCount, history, str):\n",
    "    n = len(history.split(\" \")) + 1\n",
    "    ans = 0\n",
    "    for key, val in fullFreqCount[n - 1].items():\n",
    "        # if key ends with str add 1\n",
    "        if key.endswith(str):\n",
    "            ans += 1\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kneserNeySmoothing(freqCount, history, str, fullFreqCount):\n",
    "    n = len(history.split(\" \")) + 1\n",
    "    fullString = history + \" \" + str    \n",
    "    if(history == \"\"):\n",
    "        n = 1\n",
    "        fullString = str\n",
    "    # print(\"value of n is \", n)\n",
    "    countofString = returnCount(fullString, freqCount)\n",
    "    if n == 1:\n",
    "        if countofString == 0:\n",
    "            return freqCount[0][\"<unk>\"] / sum(value for key, value in freqCount[0].items())\n",
    "        else:\n",
    "            return countofString / sum(value for key, value in freqCount[0].items())\n",
    "    countofHistory = returnCount(history, freqCount)\n",
    "    if n == 2:\n",
    "        disFactor = 0.75\n",
    "    elif n == 3:\n",
    "        disFactor = 0.9\n",
    "    \n",
    "    try:\n",
    "        lamb = disFactor * (returnPositiveCount(history, freqCount) / returnSumCount(history, freqCount))\n",
    "    except:\n",
    "        lamb = 0\n",
    "        \n",
    "    if n == 4:\n",
    "        firstTermDenominator = countofHistory\n",
    "        firstTermNumerator = max(0, countofString)\n",
    "        lamb = 0\n",
    "    else:\n",
    "        firstTermDenominator = continuationCount(fullFreqCount, history, history)\n",
    "        firstTermNumerator = max(0, continuationCount(fullFreqCount, history, history + \" \" + str) - disFactor)\n",
    "\n",
    "    if firstTermDenominator == 0:\n",
    "        # return 1/ len(freqCount[0].keys())\n",
    "        return 0.75/ freqCount[0][\"<unk>\"]\n",
    "    newHistory = \" \".join(tokenize(history)[1:])\n",
    "    # print(\"New History is \", newHistory)\n",
    "    return firstTermNumerator / firstTermDenominator + lamb * kneserNeySmoothing(freqCount, newHistory, str, fullFreqCount)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitData(text):\n",
    "    allLines = text\n",
    "    testSample = random.sample(allLines, 1000)\n",
    "    trainSample = [x for x in allLines if x not in testSample]\n",
    "    # print(testSample)\n",
    "    return trainSample, testSample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainSample, testSample = splitData(corpus2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unknowns identified\n",
      "Unknowns replaced\n"
     ]
    }
   ],
   "source": [
    "freqCount, fullFreqCount = getFrequencyCount(trainSample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get perplexity for each training sentence for a 4-gram model\n",
    "perplexity = []\n",
    "for sentence in trainSample:\n",
    "    print(\"Sentence: \", sentence)\n",
    "    tokens = tokenize(sentence)\n",
    "    prob = []\n",
    "    for i in range(len(tokens) - 3):\n",
    "        history = tokens[i] + \" \" + tokens[i + 1] + \" \" + tokens[i + 2]\n",
    "        # print(\"history: \", history)\n",
    "        prob.append(kneserNeySmoothing(freqCount, history, tokens[i + 3], fullFreqCount))\n",
    "    perplexity.append(perplexityScore(prob))\n",
    "    print(\"Perplexity is: %.3f\" % perplexityScore(prob))\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create vocabulary\n",
    "vocab = []\n",
    "for key, val in freqCount[0].items():\n",
    "    if(val > 0):\n",
    "        vocab.append(key)\n",
    "\n",
    "vocab.append(\"<unk>\")\n",
    "vocab.append(\"<start>\")\n",
    "vocab.append(\"</start>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get perplexity for testing samples\n",
    "\n",
    "perplexityTest = []\n",
    "for line in testSample:\n",
    "    tokens = tokenize(line)\n",
    "    for index in range(len(tokens)):\n",
    "        if(tokens[index] not in vocab):\n",
    "            tokens[index] = \"<unk>\"\n",
    "    tokens = addStartEnd(tokens, 3)\n",
    "\n",
    "    print(\"Sentence: \", \" \".join(tokens))\n",
    "\n",
    "    prob = []\n",
    "    for i in range(len(tokens) - 3):\n",
    "        history = tokens[i] + \" \" + tokens[i + 1] + \" \" + tokens[i + 2]\n",
    "        prob.append(kneserNeySmoothing(freqCount, history, tokens[i + 3], fullFreqCount))\n",
    "    print(\"Perplexity is: %.3f\" % perplexityScore(prob))\n",
    "    print(\"\")\n",
    "    perplexityTest.append(perplexityScore(prob))\n",
    "    print(\"Average perplexity is: %.3f\" % (sum(perplexityTest) / len(perplexityTest)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0 (main, Jan 16 2023, 14:19:54) [GCC 11.2.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "43c1af18acd4950b7883877bae71946f0f35b2e5d1cac78ed9b86fb8c35ec09a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
